---
title: "Thesis_GM_ADmetabolites_Code"
output: html_notebook
---

Package installation and loading

```{r}
## Packages (install and invoke)
pkgs <- rownames(installed.packages())
if(!"caret" %in% pkgs) install.packages("caret")
if(!"rags2ridges" %in% pkgs) install.packages("rags2ridges")
if(!"leaps" %in% pkgs) install.packages("leaps")
if(!"car" %in% pkgs) install.packages("car")
if(!"pls" %in% pkgs) install.packages("pls")
if(!"MASS" %in% pkgs) install.packages("MASS")
if(!"ISLR" %in% pkgs) install.packages("ISLR")
if(!"glmnet" %in% pkgs) install.packages("glmnet")
if(!"plotmo" %in% pkgs) install.packages("plotmo")
if(!"tree" %in% pkgs) install.packages("tree")
if(!"randomForest" %in% pkgs) install.packages("randomForest")
if(!"finalfit" %in% pkgs) install.packages("finalfit")
if(!"pROC" %in% pkgs) install.packages("pROC")
if(!"caTools" %in% pkgs) install.packages("caTools")


## Load library
library(caret)
library(dplyr)
library(rags2ridges)
library(leaps)
library(car)
library(pls)
library(glmnet)
library(plotmo)
library(tree)
library(randomForest)
library(MASS)
library(ISLR)
library(finalfit)
library(caTools)
library(Metrics)
library(FMradio)
```

Data loading

```{r}
## Invoke data, get to know objects
data(ADdata)
objects()
```

Data preprocessing

```{r}
X <- as.matrix(scale(t(ADmetabolites)))

## Replace values of ApoEClass with 0 and 1 to facilitate machine learning
y <- as.character(sampleInfo$ApoEClass)
y <- replace(y, y == 'Class 1',0)
y <- replace(y, y == 'Class 2',1)
y <- as.factor(y)
```

Projection to Latent Structures

```{r}
seed <- 123
set.seed(seed)
cov <- cor(X)

#Find redundant features
filter <- RF(cov)

#Filter out redundant features
filtered <- subSet(X, filter)

#Regularized correlation matrix estimation
M <- regcor(filtered)
```

```{r}
#Get the regularized correlation matrix of the filtered dataset
R <- M$optCor

#Get the Guttman bounds for R
Guttman.bounds <- dimGB(R)
```

```{r}
#Assess the proportion of cumulative variances for 86 factor solutions
dimVAR(R, maxdim=84)
```

```{r}
#Perform a Maximum Likelihood (ML) Factor Analysis of 84
mlfa <- mlFA(R, m=84)
thomson <- facScore(filtered,mlfa$Loadings,mlfa$Uniqueness)
```

```{r}
X <- as.matrix(thomson)
```

## Data preparation for modelling

```{r}
df <- cbind.data.frame(X,y)
```

## Ridge Logistic Regression

```{r}
# Find the lambda that minimizes the CV error
ridge_cv <- cv.glmnet(X, y, nfolds = 10,family='binomial', alpha=0, type.measure = "class")
plot(ridge_cv)
ridge_lambda <- ridge_cv$lambda.min

```

```{r}
#Use 10-fold CV to assess model performance

# Generate a shuffled sequence of row indices
nfolds <- 10

# Generate shuffled indices for the entire dataset
folds <- split(sample(nrow(df)), 1:nfolds)

#here we'll store the classification metrics for each fold
ridge.KfoldAccuracies <- numeric(nfolds)
ridge.KfoldPrecision <- numeric(nfolds)
ridge.KfoldRecall <- numeric(nfolds)
ridge.KfoldF1 <- numeric(nfolds)
ridge.KfoldAuc <- numeric(nfolds)

#the outer for loop is across the folds
for(i in 1:nfolds) {  
  #we select the objects that are to be left out in this iteration
  outofbag <- folds[[i]]
  #out of the training set
  train <- df[-outofbag,]
  X.train <- train[,1:ncol(train)-1]
  
  #and into the test set
  test <- df[outofbag,]
  X.test <- test[,1:ncol(test)-1]
  
  # Fit a logistic regression model with glmnet using ridge (alpha=0)
  ridge <- glmnet(X.train, train$y, family='binomial', alpha=0)
  
  # Predict probabilities for the test data using the selected lambda
  yhat_ridge <- as.numeric(predict(ridge, newx= as.matrix(X.test), s=ridge_lambda, type='class'))
  
  #Obtain predictions for test observations
  ridge.KfoldAccuracies[i] <- mean(yhat_ridge == test$y) #Classification accuracy
  
  # # Create a confusion matrix
  # confusion_matrix <- table(Actual=test$y, Predicted=yhat_ridge)
  # 
  # # Print the confusion matrix
  # print(confusion_matrix)
  # 
  # #Create ROC object
  # roc_ridge <- roc(as.factor(test$y), yhat_ridge)
  # plot(roc_ridge,  xlim= c(1,0),legacy.axes=TRUE, asp=0.5)
  
  #Calculate precision
  test$y <- as.numeric(test$y) -1
  ridge.KfoldPrecision[i] <- precision(test$y, yhat_ridge)
  
  #Calculate recall
  ridge.KfoldRecall[i] <- recall(test$y, yhat_ridge)
  
  #Calculate F1-score
  ridge.KfoldF1[i] <- f1(test$y, yhat_ridge)
  
  #Calculate Area under the ROC (AUC)
  ridge.KfoldAuc[i] <- auc(test$y, yhat_ridge)
}

# hist(ridge.KfoldAccuracies)
ridge.accuracy <- mean(ridge.KfoldAccuracies)
# hist(ridge.KfoldPrecision)
ridge.precision <- mean(ridge.KfoldPrecision)
# hist(ridge.KfoldRecall)
ridge.recall <- mean(ridge.KfoldRecall)
# hist(ridge.KfoldF1)
ridge.f1 <- mean(ridge.KfoldF1)
# hist(ridge.KfoldAuc)
ridge.auc <- mean(ridge.KfoldAuc)
```

Lasso Logistic Regression model

```{r}
#Use CV to find the best lambda
lasso_cv <- cv.glmnet(X, y, nfolds = 10,family='binomial', alpha=1, type.measure = "class")
plot(lasso_cv)
lasso.lambda <- lasso_cv$lambda.min
```

```{r}
# Generate a shuffled sequence of row indices
nfolds <- 10

# Generate shuffled indices for the entire dataset
folds <- split(sample(nrow(df)), 1:nfolds)

#here we'll store the classification metrics for each fold
lasso.KfoldAccuracies <- numeric(nfolds)
lasso.KfoldPrecision <- numeric(nfolds)
lasso.KfoldRecall <- numeric(nfolds)
lasso.KfoldF1 <- numeric(nfolds)
lasso.KfoldAuc <- numeric(nfolds)

#the outer for loop is across the folds
for(i in 1:nfolds) {  
  #we select the objects that are to be left out in this iteration
  outofbag <- folds[[i]]
  #out of the training set
  train <- df[-outofbag,]
  X.train <- train[,1:ncol(train)-1]
  
  #and into the test set
  test <- df[outofbag,]
  X.test <- test[,1:ncol(test)-1]
  
  # Fit a logistic regression model with glmnet using lasso (alpha=1)
  lasso <- glmnet(X.train, train$y, family='binomial', alpha=1)
  
  # Predict probabilities for the test data using the selected lambda
  yhat_lasso <- as.numeric(predict(lasso, newx= as.matrix(X.test), s=lasso.lambda, type='class'))
  
  #Obtain predictions for test observations
  lasso.KfoldAccuracies[i] <- mean(yhat_lasso == test$y) #Classification accuracy
  
  test$y <- as.numeric(test$y) -1
  
  #Calculate precision
  lasso.KfoldPrecision[i] <- precision(test$y, yhat_lasso)
  
  #Calculate recall
  lasso.KfoldRecall[i] <- recall(test$y, yhat_lasso)
  
  #Calculate F1-score
  lasso.KfoldF1[i] <- f1(test$y, yhat_lasso)
  
  #Calculate Area under the ROC (AUC)
  lasso.KfoldAuc[i] <- auc(test$y, yhat_lasso)
}

# hist(lasso.KfoldAccuracies)
lasso.accuracy <- mean(lasso.KfoldAccuracies)
# hist(lasso.KfoldPrecision)
lasso.precision <- mean(lasso.KfoldPrecision)
# hist(lasso.KfoldRecall)
lasso.recall <- mean(lasso.KfoldRecall)
# hist(lasso.KfoldF1)
lasso.f1 <- mean(lasso.KfoldF1)
# hist(lasso.KfoldAuc)
lasso.auc <- mean(lasso.KfoldAuc)
```

Elastic net Logistic Regression

```{r}
# Fit a logistic regression model with Elastic Net (alpha=0.3)
# net_model <- glmnet(X_train, y_train, family='binomial', alpha=0.3)

# Find the lambda value that minimizes the CV error for Elastic Net
net_cv <- cv.glmnet(X, y, family='binomial', alpha=0.3, type.measure = 'class')
best_lambda_net <- net_cv$lambda.min

#here we'll store the classification metrics for each fold
net.KfoldAccuracies <- numeric(nfolds)
net.KfoldPrecision <- numeric(nfolds)
net.KfoldRecall <- numeric(nfolds)
net.KfoldF1 <- numeric(nfolds)
net.KfoldAuc <- numeric(nfolds)

#the outer for loop is across the folds
for(i in 1:nfolds) {  
  #we select the objects that are to be left out in this iteration
  outofbag <- folds[[i]]
  #out of the training set
  train <- df[-outofbag,]
  X.train <- train[,1:ncol(train)-1]
  
  #and into the test set
  test <- df[outofbag,]
  X.test <- test[,1:ncol(test)-1]
  
  # Fit a logistic regression model with glmnet using elastic net penalties (mix of L1 and L2) (alpha=0.3)
  net <- glmnet(X.train, train$y, family='binomial', alpha=0.3)
  
  # Predict probabilities for the test data using the selected lambda
  yhat_net <- as.numeric(predict(net, newx= as.matrix(X.test), s=best_lambda_net, type='class'))
  
  #Obtain predictions for test observations
  net.KfoldAccuracies[i] <- mean(yhat_net == test$y) #Classification accuracy
  
  test$y <- as.numeric(test$y) -1
  #Calculate precision
  net.KfoldPrecision[i] <- precision(test$y, yhat_net)
  
  #Calculate recall
  net.KfoldRecall[i] <- recall(test$y, yhat_net)
  
  #Calculate F1-score
  net.KfoldF1[i] <- f1(test$y, yhat_net)
  library(ModelMetrics)
  #Calculate Area under the ROC (AUC)
  net.KfoldAuc[i] <- auc(test$y, yhat_net)
}

# hist(net.KfoldAccuracies)
net.accuracy <- mean(net.KfoldAccuracies)
# hist(net.KfoldPrecision)
net.precision <- mean(net.KfoldPrecision)
# hist(net.KfoldRecall)
net.recall <- mean(net.KfoldRecall)
# hist(net.KfoldF1)
net.f1 <- mean(net.KfoldF1)
# hist(net.KfoldAuc)
net.auc <- mean(net.KfoldAuc)
```

Decision Tree

```{r}
# df <- data.frame(df)
# dtree <- tree(y~., data =df)
# tree.cv <-cv.tree(dtree, FUN= prune.misclass, rand =c(40,87))

#here we'll store the classification metrics for each fold
tree.KfoldAccuracies <- numeric(nfolds)
tree.KfoldPrecision <- numeric(nfolds)
tree.KfoldRecall <- numeric(nfolds)
tree.KfoldF1 <- numeric(nfolds)
tree.KfoldAuc <- numeric(nfolds)

#the outer for loop is across the folds
for(i in 1:nfolds) {  
  #we select the objects that are to be left out in this iteration
  outofbag <- folds[[i]]
  #out of the training set
  train <- df[-outofbag,]
  #and into the test set
  test <- df[outofbag,]
  
  # Fit a decision tree 
  train$y <- as.factor(train$y)
  tree <- tree(y~., data =train)
  
  # Predict probabilities for the test data using the selected lambda
  test$y <- as.numeric(test$y)-1
  yhat_tree <- predict(tree, newdata=test, type= 'class')
  
  #Obtain predictions for test observations
  yhat_tree_n <- as.numeric(yhat_tree)-1
  tree.KfoldAccuracies[i] <- mean(yhat_tree == test$y) #Classification accuracy
  
  #Calculate precision
  tree.KfoldPrecision[i] <- precision(test$y, yhat_tree_n)
  
  #Calculate recall
  tree.KfoldRecall[i] <- recall(test$y, yhat_tree_n)
  
  #Calculate F1-score
  tree.KfoldF1[i] <- f1Score(test$y, yhat_tree_n)
  
  #Calculate Area under the ROC (AUC)
  tree.KfoldAuc[i] <- auc(test$y, yhat_tree_n)
}

# hist(tree.KfoldAccuracies)
tree.accuracy <- mean(tree.KfoldAccuracies)
# hist(tree.KfoldPrecision)
tree.precision <- mean(tree.KfoldPrecision)
# hist(tree.KfoldRecall)
tree.recall <- mean(tree.KfoldRecall)
# hist(tree.KfoldF1)
tree.f1 <- mean(tree.KfoldF1)
# hist(tree.KfoldAuc)
tree.auc <- mean(tree.KfoldAuc)
```

```{r}
dtree <- tree(y~. -y, data =df)
summary(dtree)
plot(dtree)
text(dtree)
```

Random Forest

```{r}
rf_cv <- rfcv(X, y, cv.fold = 10)

plot(rf_cv$n.var,rf_cv$error.cv)
```

```{r}
#here we'll store the classification metrics for each fold
rf.KfoldAccuracies <- numeric(nfolds)
rf.KfoldPrecision <- numeric(nfolds)
rf.KfoldRecall <- numeric(nfolds)
rf.KfoldF1 <- numeric(nfolds)
rf.KfoldAuc <- numeric(nfolds)

#the outer for loop is across the folds
for(i in 1:nfolds) {  
  #we select the objects that are to be left out in this iteration
  outofbag <- folds[[i]]
  #out of the training set
  train <- df[-outofbag,]
  train$y <- as.factor(train$y)
  #and into the test set
  test <- df[outofbag,]
  
  # Fit a Random Forest (RF)
  rf <- randomForest(y~., data= train, test= test,ntree = 400, mtry=60)
  
  # Predict probabilities for the test data using the selected lambda
  yhat_rf <- predict(rf, newdata=test)
  
  #Obtain predictions for test observations
  rf.KfoldAccuracies[i] <- mean(yhat_rf == test$y) #Classification accuracy
  
  #Calculate precision
  test$y <- as.numeric(test$y)-1
  yhat_rf <- as.numeric(yhat_rf)-1
  rf.KfoldPrecision[i] <- precision(test$y, yhat_rf)
  
  #Calculate recall
  rf.KfoldRecall[i] <- recall(test$y, yhat_rf)
  
  #Calculate F1-score
  rf.KfoldF1[i] <- f1(test$y, yhat_rf)
  
  #Calculate Area under the ROC (AUC)
  rf.KfoldAuc[i] <- auc(test$y, yhat_rf)
}

# hist(rf.KfoldAccuracies)
rf.accuracy <- mean(rf.KfoldAccuracies)
# hist(rf.KfoldPrecision)
rf.precision <- mean(rf.KfoldPrecision)
# hist(rf.KfoldRecall)
rf.recall <- mean(rf.KfoldRecall)
# hist(rf.KfoldF1)
rf.f1 <- mean(rf.KfoldF1)
# hist(rf.KfoldAuc)
rf.auc <- mean(rf.KfoldAuc)
```

```{r}
# Create a data frame to store the metrics
model_metrics_FA <- data.frame(
  Model = c("Lasso", "Ridge", "Elastic Net", "Decision Tree", "Random Forest"),
  Accuracy = c(lasso.accuracy,ridge.accuracy,net.accuracy,tree.accuracy,rf.accuracy),
  Precision = c(lasso.precision, ridge.precision, net.precision, tree.precision, rf.precision),
  Recall = c(lasso.recall, ridge.recall, net.recall, tree.recall, rf.recall),
  F1_Score = c(lasso.f1, ridge.f1, net.f1, tree.f1, rf.f1),
  AUC = c(lasso.auc, ridge.auc, net.auc, tree.auc, rf.auc)
)

# Display the table of metrics
print(model_metrics_FA)
```
