---
title: "Classification with Thomson scores"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# Classification with Thomson scores

### Package installation and loading

```{r}
## Packages (install and invoke)
pkgs <- c('caret',
          'rpart',
          'plyr',
          'dplyr',
          'xgboost',
          'ROCR',
          'FMradio',
          'rags2ridges'
          )
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
devtools::install_github("souravc83/fastAdaboost")

##Load library
library(rpart)
library(caret)
library(plyr)
library(dplyr)
library(xgboost)
library(ROCR)
library(FMradio)
library(rags2ridges)
```

### Data loading

```{r}
## Invoke data, get to know objects
data(ADdata)
```

### Data preprocessing

```{r}
X <- as.matrix(scale(t(ADmetabolites)))

```

### Projection to Latent Structures

```{r}
seed <- 123
set.seed(seed)
cov <- cor(X)

#Find redundant features
filter <- RF(cov)

#Filter out redundant features
filtered <- subSet(X, filter)

#Regularized correlation matrix estimation
M <- regcor(filtered)
```

```{r}
#Get the regularized correlation matrix of the filtered dataset
R <- M$optCor

#Get the Guttman bounds for R
Guttman.bounds <- dimGB(R)
```

```{r}
#Assess the proportion of cumulative variances for 86 factor solutions
dimVAR(R, maxdim=6)
```

```{r}
#Perform a Maximum Likelihood (ML) Factor Analysis of 84
mlfa <- mlFA(R, m=6)
thomson <- facScore(filtered,mlfa$Loadings,mlfa$Uniqueness)
```

### Data preparation for modelling

```{r}
X <- thomson

## Replace values of ApoEClass with 0 and 1 to facilitate machine learning
y <- as.character(sampleInfo$ApoEClass)
y <- replace(y, y == 'Class 1',0)
y <- replace(y, y == 'Class 2',1)
y <- as.factor(y)
```

```{r}
df <- cbind.data.frame(X,y)
set.seed(1635664)

split_index <- createDataPartition(df$y, p = 0.7, list = FALSE)

levels(df$y) <- make.names(levels(df$y))

train <- df[split_index, ]
test <- df[-split_index, ]
labels <- test$y
```

### Logistic Regression

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

## Simple logistic model with meta-features as predictors
lr <- train(y ~.,
             train,
             method = "glm",
             metric = "ROC",
             trControl = ctrl)
#Predict
yhat.lr <- as.numeric(predict(lr, test))-1
labels <- as.numeric(labels)-1

#Create a ROC curve
pred.lr <- prediction(yhat.lr,labels)
perf.lr <- performance(pred.lr, "tpr", "fpr")
plot(perf.lr,
     colorize=TRUE,
     avg='threshold',
     lwd=2,
     main='ROC curves from 10-fold cross-validation')

yhat.lr <- as.factor(yhat.lr)
labels <- as.factor(labels)

cm.lr <- confusionMatrix(labels,yhat.lr, positive='1')
metrics.lr <- data.frame(cm.lr$byClass)
auc.lr <- perf.lr@y.values[[1]]
colnames(metrics.lr) <- 'Logistic Regression'
```

### Decision Tree

```{r}
#Fit a Decision Tree
tree <- train(y ~.,
             train,
             method = "rpart2",
             tuneLength=2,
             metric = "ROC",
             trControl = ctrl)
#Predict
yhat.tree <- as.numeric(predict(tree, test))-1
labels <- as.numeric(labels)-1

#Create a ROC curve
pred.tree <- prediction(yhat.tree,labels)
perf.tree <- performance(pred.tree, "tpr", "fpr")
plot(perf.tree,
     colorize=TRUE,
     avg='threshold',
     lwd=2,
     main='ROC curves from 10-fold cross-validation')

#Get the confusion matrix and other metrics
yhat.tree <- as.factor(yhat.tree)
labels <- as.factor(labels)

cm.tree <- confusionMatrix(labels,yhat.tree, positive='1')
metrics.tree <- data.frame(cm.tree$byClass)
auc.tree <- perf.tree@y.values[[1]]
colnames(metrics.tree) <- 'Decision Tree'
```

```{r}
#Plot the tree
dtree <- tree(y~. -y, data =df)
summary(dtree)
plot(dtree)
text(dtree)
```

### Extreme gradient booster

```{r}
#Define the hyperparameter grid
tuneGrid <- expand.grid(
  nrounds = 10,            # Number of boosting iterations
  max_depth = 2,              # Maximum tree depth
  eta = 0.1,            # Learning rate (shrinkage)
  gamma =  0.2,             # Minimum loss reduction required to make a further partition
  colsample_bytree = 1, # Subsample ratio of columns
  min_child_weight = 5,     # Minimum sum of instance weight needed in a child
  subsample = 1       # Subsample percentage of the training data
)

#Fit an Extreme Gradient Boost tree
rf <- train(y ~.,
             train,
             method = "xgbTree",
             tuneGrid=tuneGrid,
             metric = "ROC",
             trControl = ctrl)
#Predict
yhat.rf <- as.numeric(predict(rf, test))-1
labels <- as.numeric(labels)-1

#Create a ROC curve
pred.rf <- prediction(yhat.rf,labels)
perf.rf <- performance(pred.rf, "tpr", "fpr")
plot(perf.rf,
     colorize=TRUE,
     avg='threshold',
     lwd=2,
     main='ROC curves from 10-fold cross-validation')

#Get the confusion matric and other metrics
yhat.rf <- as.factor(yhat.rf)
labels <- as.factor(labels)

cm.rf <- confusionMatrix(labels,yhat.rf, positive='1')
metrics.rf <- data.frame(cm.rf$byClass)
auc.rf <- perf.rf@y.values[[1]]
colnames(metrics.rf) <- 'Random Forest'
```

### Model comparison

```{r}
#Create a data frame to store the metrics
metrics <- cbind(metrics.lr,metrics.tree,metrics.rf)

auc <- cbind(auc.lr,auc.tree,auc.rf)
auc <- auc[2,]
metrics <- rbind(metrics,auc)
row.names(metrics)[12] <- 'AUC'

#Display the table of metrics
print(metrics)
```
