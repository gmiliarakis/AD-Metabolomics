---
title: "Analysis"
output:
  html_document: 
    toc: yes
    toc_depth: 4
    theme: flatly
  pdf_document:
    toc: yes
  html_notebook: 
    toc: yes
    theme: flatly
    toc_depth: 5
    highlight: pygments
editor_options:
  chunk_output_type: inline
---

```{r, include = FALSE}
require(caret)
require(dplyr)
require(e1071)
require(FMradio)
require(pROC)
require(rags2ridges)
require(xgboost)
require(rpart)
require(ggthemes)
require(DMwR2)
require(nnet)
require(fastDummies)
load("data/data.Rdata")
load("data/clinical.Rdata")
```
## RQii - Classification of metabolites on ApoE class

```{r multifit}
multifit <- function(
                X,
                y,
                model,
                ctrl = NULL,
                grid = NULL,
                seed = 87654, ...) {
  set.seed(seed)
  # Merge X and y into df
  df <- cbind.data.frame(X, y)
  # Train the model
  mdl <- caret::train(df[, 1:ncol(X)], df$y,
    method = model,
    tuneGrid = grid,
    trControl = ctrl,
    metric = "logLoss",
    ...
  )
  # Create a confusion matrix and get performance metrics from caret
  obs <- mdl$pred$obs
  preds <- mdl$pred$pred
  cm <- confusionMatrix(reference = obs, data = preds, mode = "everything")
  # Predictions
  ys <- as.numeric(obs) -1
  yhats <- as.numeric(preds) -1
  roc <- multiclass.roc(response = ys, predictor = yhats)
  out <- list("cm" = cm, "roc" = roc, "model" = mdl)
  return(out)
}
```

### Using 230 metabolites
```{r multi_preparation}
X <- scale(df[, 1:230])
y <- df$target
levels(y) <- make.names(levels(y))

clinical$L_PTAU = as.numeric(clinical$L_PTAU)
require(mice)
imp <- mice(clinical, method = "norm", seed = 123)
clinical <- complete(imp)

clin_dummy <- dummy_cols(clinical, remove_first_dummy = T, remove_selected_columns = T)
rownames(clin_dummy) = rownames(clinical)
clin_dummy$V_MMSE = NULL

Xclin = cbind.data.frame(X, clin_dummy)

clin_dummy$V_MMSE = NULL
full = merge(X, clin_dummy, by= 0)
rownames(full) = full$Row.names
full$Row.names = NULL
ctrl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 10,
  savePredictions = "final",
  classProbs = T,
  summaryFunction = multiClassSummary,
  selectionFunction = best,
  search = "random",
  sampling = "smote"
)
```

#### Penalised Multinomial Logistic Regression

```{r mlogit}
clin_only <- multifit(
  X = clin_dummy,
  y = y,
  model = "multinom",
  ctrl = ctrl,
  tuneLength = 10,
  trace = F
)
clin_only$cm

mlr <- multifit(
  X = Xclin,
  y = y,
  model = "multinom",
  ctrl = ctrl,
  tuneLength = 10,
  trace = F
)
```

#### Decision Tree

```{r tree}
tree <- multifit(
  X = X,
  y = y,
  model = "rpart2",
  ctrl = ctrl,
  grid = expand.grid(maxdepth = 2),
  preProcess = c("center", "scale", "corr")
)
tree$cm
```

#### Random Forest

```{r xgb}
rf <- multifit(
  X = X,
  y = y,
  model = "xgbTree",
  ctrl = ctrl,
  tuneLength = 1,
  preProcess = c("center", "scale", "corr")
)
rf$cm
```

### Projection to Latent Factors

```{r get_thomson}
project <- function(X, m, seed) {
  set.seed(seed)
  X <- scale(as.matrix(df[,1:230]))
  cov <- cor(X)

  # Find redundant features
  filter <- RF(cov)

  # Filter out redundant features
  filtered <- subSet(X, filter)

  # Regularized correlation matrix estimation
  M <- regcor(filtered)

  # Get the regularized correlation matrix of the filtered dataset
  R <- M$optCor

  mlfa <- mlFA(R, m = 6)
  thomson <- facScore(filtered, mlfa$Loadings, mlfa$Uniqueness)
  return(thomson)
}
```

```{r}
X <- project(X, 6, seed = 1234)
```
#### Multinomial Logistic Regression
```{r }
# X <- cbind(clnr, thomson)
mlrf <- multifit(
  X = X,
  y = y,
  model = "multinom",
  ctrl = ctrl,
  trace = FALSE,
  grid = expand.grid(decay = 0)
)
mlrf$cm
plot.roc(mlrf$roc)
```

#### Decision Tree

```{r multi_tree}
mtreef <- multifit(
  X = X,
  y = y,
  model = "rpart2",
  ctrl = ctrl,
  grid = tree$model$bestTune
)
mtreef$cm
```

#### XGBoost Forest

```{r }
mrff <- multifit(
  X = X,
  y = y,
  model = "xgbTree",
  ctrl = ctrl,
  tuneLength = 1
)
mrff$cm
```