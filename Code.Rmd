---
title: "Analysis"
output:
  pdf_document:
    toc: yes
  html_notebook:
    toc: yes
editor_options:
  chunk_output_type: inline
---

# Analysis

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  comment = '', fig.width = 6, fig.height = 6, echo = FALSE
)
```

## Package installation and loading

```{r, packages, echo=FALSE}
require(caret)
require(FMradio)
require(rags2ridges)
require(rpart)
require(randomForest)
require(ROCR)
```

## Data loading

```{r, 'data loading'}
# Invoke data
data(ADdata)
```

## Differential expression of metabolites per ApoE genotype

### Data Preparation

```{r, Y}
# Transpose, standardize and store the metabolite data in Y
Y <- as.matrix(scale(t(ADmetabolites)))
```

```{r,  'x,df'}
# Store the ApoE genotype in x as a facgor
x <- as.factor(sampleInfo$ApoEClass)
df <- cbind.data.frame(Y,x)
```

### Analysis of Variance (ANOVA)

```{r,  aov}
# Store the ANOVA model summaries in summaries
summaries <- purrr::map(df[,1:230], ~summary(aov(.x ~ df$x)))
```

#### Correction for Multiple Testing

```{r, 'FDR adjust', echo=FALSE}
#Extract and adjust p-values for FDR control

#Create a list to store the p-values
p_values <- list(1:230)

#Extract the p-values of the F-tests from the aov summaries matrix and store them in p_values
for (i in 1:230){
  p_values[[i]] <- summaries[[i]][[1]][["Pr(>F)"]][[1]]
}

# Coerce p_values to dataframe and transpose it 
p_values <- t(data.frame(p_values))

# Set as row names the metabolites
p_values <- data.frame(p_values, row.names = colnames(Y))

# Calculate the FDR-adjusted p-values
p_values$p_adj <- p.adjust(p_values[,], method = "fdr")

# Filter out the non-significant (a=0.05) FDR-adjusted p-values
p_values <- filter(p_values, p_adj <= 0.05)

p_values
```

## Classification of metabolites based on ApoE class

A function was created to streamline the analysis of multiple (tidy tabular standardised) predictors and response. The function *fit* takes the arguments *X = data frame of predictors* and *Y = response*, fits a Logistic Regression, a Decision Tree and a (bagging) Random Forest. It subsequently creates a dataframe with the repeated 10-fold Cross-Validation(CV) model performance evaluation metrics and plots the ROC curves.

```{r, 'fit function def'}
fit <- function(X = NULL,
                Y = NULL,
                max_depth= 4,
                mtry= 6,
                seed= 123) {
  
  # Replace values of ApoEClass with 0 and 1 to facilitate machine learning
  y <- as.factor(x)
  
  #Encode y so valid R variables can be created from its classes
  levels(y) <- make.names(levels(y))
  
  ### Training-test split
  #Merge X and y into df
  df <- cbind.data.frame(X, y)
  set.seed(seed)
  
  #Create a matrix with the training indexes
  split_index <- createDataPartition(df$y, p = 0.7, list = FALSE)
  
  #Create the training set
  train <- df[split_index,]
  
  #Create the test set
  test <- df[-split_index,]
  
  #Store the test y in labels
  labels <- test$y
  labels.n <- as.numeric(labels) - 1
  
  #Define the training control hyperparameters
  ctrl <- trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 10,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  )
  
  ### Logistic Regression
  set.seed(seed)
  
  lr <- train(y ~ .,
              train,
              method = "glm",
              metric = "ROC",
              trControl = ctrl)
  #Predict
  yhat.lr <- predict(lr, test)
  
  #Create a confusion matrix and get performance metrics from caret
  cm.lr <- confusionMatrix(labels, yhat.lr, positive = 'Class.2')
  
  #Plot ROC curve for LR
  yhat.lr <- as.numeric(yhat.lr) -1
  pred.lr <- prediction(yhat.lr, labels.n)
  perf.lr <<- performance(pred.lr, "tpr", "fpr")
  plot(perf.lr,
    col=1,
    lwd = 2,
    main = 'ROC curves from repeated 10-fold CV'
  )
  
  #Create a data frame to store the performance metrics
  metrics.lr <<- data.frame(cm.lr$byClass)
  auc.lr <<- perf.lr@y.values[[1]][2]
  colnames(metrics.lr) <- 'Logistic Regression'
  
  ### Decision Tree
  
  # Define the tree training hyperparameters
  tree <- train(
    y ~ .,
    train,
    method = "rpart2",
    tuneLength = max_depth,
    metric = "ROC",
    trControl = ctrl
  )
  #Predict
  yhat.tree <- predict(tree, test)
  
  #Create a confusion matrix and get metrics
  cm.tree <- confusionMatrix(labels, yhat.tree, positive = 'Class.2')
  
  #Create a ROC curve
  yhat.tree <- as.numeric(yhat.tree) - 1
  pred.tree <- prediction(yhat.tree, labels.n)
  perf.tree <<- performance(pred.tree, "tpr", "fpr")
  plot(perf.tree,
    col=2,
    lwd = 2,
    add = TRUE
  )
  
  #Store metrics in a new dataframe
  metrics.tree <<- data.frame(cm.tree$byClass)
  auc.tree <<- perf.tree@y.values[[1]][2]
  colnames(metrics.tree) <- 'Decision Tree'
  
  
  ### Random Forest
  rf <- train(
    y ~ .,
    train,
    method = "rf",
    tuneLength = mtry,
    metric = "ROC",
    trControl = ctrl
  )
  #Predict
  yhat.rf <- predict(rf, test)

  #Get the confusion matrix and other metrics
  cm.rf <- confusionMatrix(labels,yhat.rf, positive='Class.2')

  #Create a ROC curve
  yhat.rf <- as.numeric(yhat.rf) -1
  pred.rf <- prediction(yhat.rf, labels.n)
  perf.rf <- performance(pred.rf, "tpr", "fpr")
  plot(
    perf.rf,
    col= 3,
    lwd = 2,
    add= TRUE
  )
  legend("bottomright", c("LR", "DT","RF"), lwd=2, 
    col = c(1:3), bty="n")
  #Store metrics in a new dataframe
  metrics.rf <- data.frame(cm.rf$byClass)  
  #Add AUC
  auc.rf <- perf.rf@y.values[[1]][2]
  colnames(metrics.rf) <- 'Random Forest'
  ### Model comparison
  
  #Create a data frame to store and compare the metrics
  metrics <<- cbind(metrics.lr, metrics.tree, metrics.rf)
  metrics[12,] <- auc <<- c(auc.lr, auc.tree, auc.rf)
  row.names(metrics)[12] <- 'AUC'
  
  #Display the table of metrics
  return(metrics)
}
```

### Original data

```{r, 'fit original data'}
#Invoke the function
X <- Y
fit(X = X,
    Y = sampleInfo,
    max_depth = 2,
    mtry=5)
```

### Using ML-estimated factor scores

```{r, 'filter X'}
seed <- 123
set.seed(seed)
cov <- cor(X)

#Find redundant features
filter <- RF(cov)

#Filter out redundant features
filtered <- subSet(X, filter)

#Regularized correlation matrix estimation
M <- regcor(filtered)
```

```{r, optCor}
#Get the regularized correlation matrix of the filtered dataset
R <- M$optCor

#Get the Guttman bounds for R
Guttman.bounds <- dimGB(R)
```

```{r, dimVar}
#Assess the proportion of cumulative variances for 6 factor solutions
dimVAR(R, maxdim=6)
```

#### Maximum Likelihood (ML) Factor Analysis with 6 factors

```{r, 'mlfa thomson'}
mlfa <- mlFA(R, m=6)
thomson <- facScore(filtered,mlfa$Loadings,mlfa$Uniqueness)
```

```{r, 'fit thomson'}
fit(X = thomson,
    Y = sampleInfo)
```

## Network Analysis

```{r, 'C1, C2, Slist, Tlist, Ylist, optf, P0s'}
# Store all observations of Class 1 in C1
C1 <- scale(t(ADmetabolites[, sampleInfo$ApoEClass == "Class 1"]))
# Store all observations of Class 2 in C2
C2 <- scale(t(ADmetabolites[, sampleInfo$ApoEClass == "Class 2"]))

#Get the covariance matrices of C1 and C2
S1<- covML(C1)
S2<- covML(C2)

#Store them in a list
Slist<- list(S1 = S1, S2 = S2)

#Get the total number of samples
n <- c(nrow(S1), nrow(S2))

# Create a list of fused covariance matrices T
Tlist<- default.target.fused(Slist = Slist, ns = n, type = "DUPV")

# Create a list of the two-class data Y
Ylist<- list(C1 = C1, C2 = C2)


# Get the optimal lambdas per class and fused with 10-fold CC
set.seed(8910)
optf <- optPenalty.fused(Ylist = Ylist,
                         Tlist = Tlist,
                         lambda = as.matrix(cbind(c("ridge1", "fusion"),
                                                  c("fusion", "ridge2"))),
                         cv.method = "kCV",
                         k = 10,
                         verbose = FALSE)
# Sparsify the l2-regularised high precision matrices, correcting for FDR at .001
P0s <- sparsify.fused(optf$Plist,
                      threshold = "localFDR",
                      FDRcut = 0.999,
                      verbose = FALSE)
```

```{r, 'GGMs per class and diff'}
#Merge the sparse high precision matrices
TST <- Union(P0s$C1$sparseParCor, P0s$C2$sparseParCor)
PCclass1 <- TST$M1subset
PCclass2 <- TST$M2subset

# Create a color map
Colors <- rownames(PCclass2)
Colors[grep("Amine", rownames(PCclass2))]     <- "lightblue"
Colors[grep("Org.Acid", rownames(PCclass2))]  <- "orange"
Colors[grep("Lip", rownames(PCclass2))]       <- "yellow"
Colors[grep("Ox.Stress", rownames(PCclass2))] <- "purple"


set.seed(111213)
opar <- par(mfrow = c(1, 3))
#Plot the sparsified ridge matrix of AD Class 2
Coords <- Ugraph(PCclass2, type = "fancy", lay = "layout_with_fr",
    Vcolor = Colors, prune = FALSE, Vsize = 7, Vcex = 0.3,
    main = "AD Class 2")
#Plot he sparsified ridge matrix of AD Class 1
Ugraph(PCclass1, type = "fancy", lay = NULL, coords = Coords,
    Vcolor = Colors, prune = FALSE, Vsize = 7, Vcex = 0.3,
    main = "AD Class 1")

#Plot the differential network
DiffGraph(PCclass1, PCclass2, lay = NULL, coords = Coords,
    Vcolor = Colors, Vsize = 7, Vcex = 0.3,
    main = "Differential Network")
par(opar)
```

```{r, 'degree scores'}
PC0list      <- list(PCclass1 = PCclass1, PCclass2 = PCclass2)
# Get the network statistics
NetStatsList <- GGMnetworkStats.fused(PC0list)

# Get the centrality degree scores for each class
DegreesAD1   <- data.frame(rownames(NetStatsList),NetStatsList$PCclass1.degree)
DegreesAD2   <- data.frame(rownames(NetStatsList),NetStatsList$PCclass2.degree)

# Order and show the centrality degree scores
DegreesAD1o  <- DegreesAD1[order(DegreesAD1[, 2], decreasing = TRUE), ]
DegreesAD2o  <- DegreesAD2[order(DegreesAD2[, 2], decreasing = TRUE), ]
head(DegreesAD1o, 7)
head(DegreesAD2o, 7)
```

```{r, 'communities per class'}
# Get the communities per class
set.seed(141516)
opar <- par(mfrow = c(1, 2))
CommC1 <- Communities(PCclass1, Vcolor = Colors, Vsize = 7,  Vcex = 0.3, main = "Modules AD Class 1")

CommC2 <- Communities(PCclass2, Vcolor = Colors, Vsize = 7,  Vcex = 0.3, main = "Modules AD Class 2")
par(opar)
```

```{r, 'degree density plot'}
# Plot the densities of centrality degree scores
plot(density(DegreesAD1[, 2]), col = "blue", xlim = c(-1, 8), xlab = "Degree", main = "")
lines(density(DegreesAD2[, 2]), col = "red")
legenda <- c("AD class 1", "AD class 2")
legend(5, 0.5, legend = legenda, lwd = rep(1, 2), lty = rep(1, 2), col = c("blue", "red"), cex = 0.7)
```

```{r, 'wilcoxon rank sum'}
#Perform a Wilcoxon signed rank test to test if the centrality degree scores are different between the classes
wilcox.test(DegreesAD1[, 2], DegreesAD2[, 2], paired = TRUE, alternative = "less")
```
