---
title: "Analysis"
output:
  html_document: 
    toc: yes
    toc_depth: 4
    theme: darkly
    fig_width: 6
  pdf_document:
    toc: yes
  html_notebook: 
    toc: yes
    theme: darkly
    toc_depth: 5
    highlight: pygments
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  comment = "", fig.width = 6, fig.height = 6, warning = FALSE
)
```

```{r packages, include=FALSE}
require(caret)
require(dplyr)
require(e1071)
require(FMradio)
require(pROC)
require(rags2ridges)
require(xgboost)
require(ranger)
require(ggthemes)
require(DMwR2)
```

## Data loading

```{r data loading, echo=FALSE}
# Invoke data
load("data.Rda")
```

## Differential expression of metabolites per ApoE genotype

### Data Preparation

```{r Y}
# Transpose, standardize and store the metabolite data in Y
load('data.Rda')
Y <- as.matrix(ADmets)
```
### Wilcoxon rank-sum test
Triglycerides and diglycerides seem to survive FDR control.
```{r wilcoxon E4yes vs E4no}
# Store all observations of Class 1 in C1
C1 <- ADmets[geno$APOEb == "E4NO",1:230]
# Store all observations of Class 2 in C2
C2 <- ADmets[geno$APOEb == "E4YES", 1:230]

# Create a function to perform the Wilcoxon rank-sum (Mann-Whitney U) test on two vectors
MannWhitneyU <- function(x, y) {
  wilcoxon <- wilcox.test(x, y, paired = FALSE, alternative = "less")
  return(wilcoxon$p.value)
}

# Use purrr::map2 to apply the function to corresponding columns
wilcoxons <- purrr::map2(C1[,1:230], C2[,1:230], ~ MannWhitneyU(.x,.y))

# Coerce p_values to dataframe and transpose it
p_values <- t(data.frame(wilcoxons))
# Calculate the FDR-adjusted p-values
p_adj <- p.adjust(wilcoxons, method = "fdr")

results <- data.frame(p_values, p_adj)
# Filter out the non-significant (a=0.05) FDR-adjusted p-values
dplyr::filter(results, p_adj < 0.05)
```
Carriers of one copy of E4 vs two copies tend to have less histamine, fumaric acid, uracil, triglyceride TG.48.0, phosphatidylcholine PC.36.4, with p < 0.05, however these don't survive FDRcut at 0.95 (p_adj = 0.97)'

```{r E4x1 vs E4x2}
geno$g <- geno$APOE
geno$g[geno$g == "E3E4"] <- geno$g[geno$g == "E2E4"] <- "E4x1"
geno$g[geno$g == "E4E4"] <- 'E4x2'
# Store all observations of Class 1 in C1
C1 <- ADmets[geno$g == "E4x1",1:230]
# Store all observations of Class 2 in C2
C2 <- ADmets[geno$g == 'E4x2', 1:230]

# Use purrr::map2 to apply the function to corresponding columns
wilcoxons <- purrr::map2(C1[, 1:230], C2[, 1:230], ~ MannWhitneyU(.x, .y))

# Coerce p_values to dataframe and transpose it
p_values <- t(data.frame(wilcoxons))
# Calculate the FDR-adjusted p-values
p_adj <- p.adjust(wilcoxons, method = "fdr")

results <- data.frame(p_values, p_adj)
# Filter out the non-significant (a=0.05) FDR-adjusted p-values
dplyr::filter(results, p_adj < 0.05)

```

```{r E4no vs E4x1}
geno$g <- geno$APOE
geno$g[geno$g == "E3E4"] <- geno$g[geno$g == "E2E4"] <- "E4x1"
geno$g[geno$APOEb == "E4NO"] <- 'E4x0'
# Store all observations of Class 1 in C1
C1 <- ADmets[geno$g == "E4x1",1:230]
# Store all observations of Class 2 in C2
C2 <- ADmets[geno$g == 'E4x0', 1:230]

# Use purrr::map2 to apply the function to corresponding columns
wilcoxons <- purrr::map2(C1[, 1:230], C2[, 1:230], ~ MannWhitneyU(.x, .y))

# Coerce p_values to dataframe and transpose it
p_values <- t(data.frame(wilcoxons))
# Calculate the FDR-adjusted p-values
p_adj <- p.adjust(wilcoxons, method = "fdr")

results <- data.frame(p_values, p_adj)
# Filter out the non-significant (a=0.05) FDR-adjusted p-values
dplyr::filter(results, p_adj < 0.05)
```

## Classification of metabolites on ApoE class

```{r}
## Function fit
fit <- function(title,
                X,
                y,
                model,
                ctrl = NULL,
                grid = NULL,
                seed = 123, ...) {
                set.seed(seed)
    # Merge X and y into df
    df <- merge.data.frame(X, y)

  # Train the model
  mdl <- caret::train(df[, 1:ncol(X)], df$y,
    method = model,
    tuneGrid = grid,
    trControl = ctrl,
    metric = 'ROC',...
  )

  # Create a confusion matrix and get performance metrics from caret
  metrics <- confusionMatrix(mdl$pred$obs, mdl$pred$pred, positive = "E4YES")$byClass

  # Predictions
  ys <- as.numeric(mdl$pred$obs) -1
  yhats <- mdl$pred$E4YES
  roc <- roc(ys, yhats, levels = c(0, 1), ci = TRUE, boot.n = 1000, ci.alpha = 0.95)
  metrics[12] <-  roc$auc
  row.names(metrics)[12] = 'AUC'
  names(metrics) <- title
  out <- list("metrics" = metrics, "roc" = roc, "model" = mdl)
  return(out)
}

```


### Original data

```{r fit_original}
k <- 1
y <- apoe
require(glmnet)
X <- scale(df[1:230])
#  Logistic Regression
lr <- train(
  x = X,
  y = apoe,
  model = "xgbLinear",
  metric = 'ROC',
  trControl = ctrl,
  tuneGrid= param,
  times = k
) 
 
## Decision Tree
param <-  data.frame(nrounds=c(10), max_depth = c(2),eta =c(0.3),gamma=c(0), colsample_bytree=c(0.5),min_child_weight=c(1),subsample=c(1))
tree <- fit(
  title = "Decision Tree",
  X = X,
  y = y,
  model = "xgbTree",
  grid = param,
  times = k
)
ctrl <- trainControl(
                  method = "boot", 
                  number = 100,
                  classProbs = TRUE, 
                  summaryFunction = twoClassSummary, 
                  savePredictions = TRUE
                )
xgbtree <- train(ADmets, apoe, method = "xgbTree", metric = "ROC", trControl = ctrl, tuneGrid = param, verbose = FALSE)

linear <- train(X, apoe, method='xgbLinear', metric = 'ROC', trControl = ctrl, tuneLength=2, verbose = FALSE)

## Random Forest
rf <- train(
x = ADmets,
  y = apoe,
  model = "ranger",
  trControl = ctrl,
  metric = 'ROC',
                 tuneGrid = expand.grid(mtry = 5),
  num.trees = 50
)
ys <- as.numeric(mdl$pred$obs) -1
yhats <- mdl$pred$E4YES

metrics <- cbind(lr$metrics, tree$metrics, rf$metrics)
rocs <- list(lr$roc, tree$roc, rf$roc)

# Generate labels
labels <- paste0(names(metrics), ", AUC = ", paste(round(metrics[12, ], 2)))
# plot on a single plot with AUC in labels
# httpgd::hgd()
ggroc(rocs) +
  theme_clean() +
  scale_color_tableau(labels=labels)
```

### Projection to Latent Factors

```{r get_thomson, eval =FALSE, include=FALSE}
project <- function(X, m, seed) {
  set.seed(seed)
  X <- scale(as.matrix(X))
  cov <- cor(X)
  
  # Find redundant features
  filter <- RF(cov)
  
  # Filter out redundant features
  filtered <- subSet(X, filter)
  
  # Regularized correlation matrix estimation
  M <- regcor(filtered)
  
  # Get the regularized correlation matrix of the filtered dataset
  R <- M$optCor
  
  mlfa <- mlFA(R, m = m)
  thomson <- facScore(filtered, mlfa$Loadings, mlfa$Uniqueness)
  return(thomson)
}
```

```{r fit_thomson, warning=FALSE}
#Set k for repeats
k <- 10
## Logistic Regression
lrf <- fit(
  title = "Logistic Regression",
  X = thomson,
  y = apoe,
  model = "glm",
  times = k,
  grid = NULL
)

## Decision Tree
treef <- fit(
  title = "Decision Tree",
  X = thomson,
  y = apoe,
  model = "rpart2",
  grid = expand.grid(maxdepth = 3),
  times = k
)

## Ensemble Classifiers
rff <- fit(
  title = "Random Forest",
  X = thomson,
  y = apoe,
  model = "ranger",
  ctrl = trainControl(
                  method = "cv", number = 10,
                  classProbs = TRUE, summaryFunction = twoClassSummary,
                  sampling = "smote"
                ),
                grid = NULL,
  tuneLength = 2,
  times = 10,
  num.trees = 50
)

metricsf <- cbind(lrf$metrics, treef$metrics, rff$metrics)
rocsf <- list(lrf$roc, treef$roc, rff$roc)

# plot on a single plot with AUC in labels
ggroc(rocsf) +
  theme_clean() +
  scale_color_tableau(labels= labels)
```

## Network Analysis

Code from rags2ridges

```{r preparation}
# Store all observations of Class 1 in C1
C1 <- scale(df[df$APOE == "E4NO",1:230])
# Store all observations of Class 2 in C2
C2 <- scale(df[APOE == "E4YES",1:230])

# Get the covariance matrices of C1 and C2
S1 <- covML(C1)
S2 <- covML(C2)

# Store them in a list
S <- list(S1 = S1, S2 = S2)

# Get the total number of samples
n <- c(nrow(S1), nrow(S2))

# Create a list of fused covariance matrices T
Ts <- default.target.fused(Slist = S, ns = n, type = "DUPV")
```

```{r, eval=FALSE}
# Get the optimal lambdas per class and fused with 10-fold CV
set.seed(8910)
# optf <- optPenalty.fused(
#   Ylist = Ys,
#   Tlist = Ts,
#   lambda = as.matrix(cbind(
#     c("ridge1", "fusion"),
#     c("fusion", "ridge2")
#   )),
#   cv.method = "kCV",
#   k = 10,
#   verbose = FALSE
# )
```

```{r sparsify, echo = FALSE}
# Create a list of the two-class data Y
Ys <- list(C1 = C1, C2 = C2)
Ps <- optP.fused$Plist
# Get the sparsified high precision matrices, correcting FDR at .001
P0s <- sparsify.fused(Ps,
  threshold = "localFDR",
  FDRcut = 0.999,
  verbose = FALSE
)
```

```{r GGMs per class and diff, message=FALSE, echo = FALSE}
# Merge the sparse high precision matrices
TST <- Union(P0s$C1$sparseParCor, P0s$C2$sparseParCor)
PCclass1 <- TST$M1subset
PCclass2 <- TST$M2subset

# Create a color map per metabolite class
Colors <- rownames(PCclass2)
Colors[grep("Amine", rownames(PCclass2))] <- "lightblue"
Colors[grep("Org.Acid", rownames(PCclass2))] <- "orange"
Colors[grep("Lip", rownames(PCclass2))] <- "yellow"
Colors[grep("Ox.Stress", rownames(PCclass2))] <- "purple"


set.seed(111213)
opar <- par(mfrow = c(1, 3))
# Plot the sparsified ridge matrix of AD Class 2
Coords <- Ugraph(PCclass2,
  type = "fancy", lay = "layout_with_fr",
  Vcolor = Colors, prune = FALSE, Vsize = 7, Vcex = 0.3,
  main = "AD Class 2"
)
# Plot he sparsified ridge matrix of AD Class 1
Ugraph(PCclass1,
  type = "fancy", lay = NULL, coords = Coords,
  Vcolor = Colors, prune = FALSE, Vsize = 7, Vcex = 0.3,
  main = "AD Class 1"
)

# Plot the differential network
DiffGraph(PCclass1, PCclass2,
  lay = NULL, coords = Coords,
  Vcolor = Colors, Vsize = 7, Vcex = 0.3,
  main = "Differential Network"
)
par(opar)
```

```{r centrality}
PC0list <- list(PCclass1 = PCclass1, PCclass2 = PCclass2)
# Get the network statistics
NetStats <- GGMnetworkStats.fused(PC0list)

# Get the centrality degree scores for each class
DegreesAD1 <- data.frame(rownames(NetStats), NetStats$PCclass1.degree)
DegreesAD2 <- data.frame(rownames(NetStats), NetStats$PCclass2.degree)

# Order and show the centrality degree scores
DegreesAD1o <- DegreesAD1[order(DegreesAD1[, 2], decreasing = TRUE), ]
DegreesAD2o <- DegreesAD2[order(DegreesAD2[, 2], decreasing = TRUE), ]
head(DegreesAD1o, 7)
head(DegreesAD2o, 7)
```

```{r communities}
# Get the communities per class
set.seed(141516)
opar <- par(mfrow = c(1, 2))
CommC1 <- Communities(PCclass1,
  Vcolor = Colors,
  Vsize = 7, Vcex = 0.3, main = "Modules AD Class 1"
)

CommC2 <- Communities(PCclass2,
  Vcolor = Colors,
  Vsize = 7, Vcex = 0.3, main = "Modules AD Class 2"
)
par(opar)
```

```{r degree density plot }
# Plot the densities of centrality degree scores
plot(density(DegreesAD1[, 2]),
  col = "blue", xlim = c(-1, 8), xlab = "Degree", main = ""
)
lines(density(DegreesAD2[, 2]),
  col = "red"
)
legenda <- c("AD class 1", "AD class 2")
legend(5, 0.5,
  legend = legenda, 
  lwd = rep(1, 2), lty = rep(1, 2), col = c("blue", "red"), cex = 0.7
)
```

```{r wilcoxon rank sum }
# Perform a Wilcoxon signed rank test to test if the centrality degree scores are different between the classes
wilcox.test(DegreesAD1[, 2],
  DegreesAD2[, 2],
  paired = TRUE, alternative = "less"
)
```
