---
title: "Classification with original data"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# Classification with the original data

### Package installation and loading

```{r}
#Packages (install and invoke)
pkgs <- c('car',
          'caret',
          'leaps',
          'glm2',
          'MASS',
          'ISLR',
          'tree',
          'xgboost',
          'ROCR',
          'caTools',
          'plotmo',
          'FMradio',
          "RCurl",
          "jsonlite",
          'rags2ridges')
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}


#Load library
library(car)
library(caret)
library(glm2)
library(dplyr)
library(rags2ridges)
library(tree)
library(xgboost)
library(FMradio)
library(ROCR)
```

### Data loading

```{r}
#Invoke data, get to know objects
data(ADdata)
```

## Data preparation for modelling

```{r}
X <- as.matrix(scale(t(ADmetabolites)))
```

```{r}
# Replace values of ApoEClass with 0 and 1 to facilitate machine learning
y <- as.character(sampleInfo$ApoEClass)
y <- replace(y, y == 'Class 1',0)
y <- replace(y, y == 'Class 2',1)
y <- as.factor(y)
```

### Training-test split

```{r}
#Merge X and y into df
df <- cbind.data.frame(X,y)
set.seed(1635664)

#Create a matrix with the training indexes
split_index <- createDataPartition(df$y, p = 0.7, list = FALSE)

#Encode y so valid R variables can be created from its classes
levels(df$y) <- make.names(levels(df$y))

#Create the training set
train <- df[split_index, ]

#Create the test set
test <- df[-split_index, ]

#Store the test y in labels
labels <- test$y
```

### Logistic Regression

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 10,
                     classProbs = TRUE,
                     summaryFunction = twoClassSummary)

# Simple logistic model with meta-features as predictors
set.seed(1635664)

lr <- train(y ~.,
             train,
             method = "glm",
             metric = "ROC",
             trControl = ctrl)
#Predict
yhat.lr <- as.numeric(predict(lr, test))-1
labels <- as.numeric(labels)-1

#Plot ROC curve
pred.lr <- prediction(yhat.lr,labels)
perf.lr <- performance(pred.lr, "tpr", "fpr")
plot(perf.lr,
     colorize=TRUE,
     avg='threshold',
     lwd=2,
     main='ROC curves from 10-fold cross-validation')

#Create a confusion matrix and get performance metrics from caret
yhat.lr <- as.factor(yhat.lr)
labels <- as.factor(labels)

cm.lr <- confusionMatrix(labels,yhat.lr, positive='1')

#Create a data drame to store the performance metrics
metrics.lr <- data.frame(cm.lr$byClass)
auc.lr <- perf.lr@y.values[[1]]
colnames(metrics.lr) <- 'Logistic Regression'
```

### Decision Tree

```{r}
tree <- train(y ~.,
             train,
             method = "rpart2",
             tuneLength=4,
             metric = "ROC",
             trControl = ctrl)
#Predict
yhat.tree <- as.numeric(predict(tree, test))-1
labels <- as.numeric(labels)-1

#Create a ROC curve
pred.tree <- prediction(yhat.tree,labels)
perf.tree <- performance(pred.tree, "tpr", "fpr")
plot(perf.tree,
     colorize=TRUE,
     avg='threshold',
     lwd=2,
     main='ROC curves from 10-fold cross-validation')

#Create a confusion matrix and get metrics
yhat.tree <- as.factor(yhat.tree)
labels <- as.factor(labels)

cm.tree <- confusionMatrix(labels,yhat.tree, positive='1')

#Store in a new dataframe
metrics.tree <- data.frame(cm.tree$byClass)
auc.tree <- perf.tree@y.values[[1]]
colnames(metrics.tree) <- 'Decision Tree'
```

### Extreme Gradient Booster

```{r}
tuneGrid <- expand.grid(
  nrounds = 10,            # Number of boosting iterations
  max_depth = 2,              # Maximum tree depth
  eta =  0.1,            # Learning rate (shrinkage)
  gamma =  0.2,             # Minimum loss reduction required to make a further partition
  colsample_bytree = 1, # Subsample ratio of columns
  min_child_weight = 1,    # Minimum sum of instance weight needed in a child
  subsample = 1       # Subsample percentage of the training data
)
rf <- train(y ~.,
             train,
             method = "xgbTree",
             tuneGrid = tuneGrid,
             metric = "ROC",
             trControl = ctrl)
#Predict
yhat.rf <- as.numeric(predict(rf, test))-1
labels <- as.numeric(labels)-1

#Create a ROC curve
pred.rf <- prediction(yhat.rf,labels)
perf.rf <- performance(pred.rf, "tpr", "fpr")
plot(perf.rf,
     colorize=TRUE,
     avg='threshold',
     lwd=2,
     main='ROC curves from 10-fold cross-validation')

#Get the confusion matrix and other metrics
yhat.rf <- as.factor(yhat.rf)
labels <- as.factor(labels)
auc.rf <- perf.rf@y.values[[1]]

cm.rf <- confusionMatrix(labels, yhat.rf, positive='1')
metrics.rf <- data.frame(cm.rf$byClass)
colnames(metrics.rf) <- 'Random Forest'
```

### Model comparison

```{r}
#Create a data frame to store and compare the metrics
metrics <- cbind(metrics.lr,metrics.tree,metrics.rf)

auc <- cbind(auc.lr,auc.tree,auc.rf)
auc <- auc[2,]
metrics <- rbind(metrics,auc)
row.names(metrics)[12] <- 'AUC'

#Display the table of metrics
print(metrics)
```
