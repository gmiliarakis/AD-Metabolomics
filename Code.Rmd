---
title: "Analysis"
output:
  html_document: 
    toc: yes
    toc_depth: 4
    theme: journal
    fig_width: 6
  pdf_document:
    toc: yes
  html_notebook: 
    toc: yes
    theme: flatly
    toc_depth: 5
    highlight: pygments
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  comment = "", fig.width = 6, fig.height = 6, warning = FALSE
)
```

```{r packages, include=FALSE}
require(caret)
require(dplyr)
require(FMradio)
require(rags2ridges)
require(rpart)
require(randomForest)
require(pROC)
require(h2o)
```

## Data loading

```{r data loading, echo=FALSE}
# Invoke data
load("ADdata.Rdata")
```

## Differential expression of metabolites per ApoE genotype

### Data Preparation

```{r Y}
# Transpose, standardize and store the metabolite data in Y
Y <- as.matrix(ADmets)
```

```{r df}
# Store the ApoE genotype in APOE as a factor
APOE <- as.factor(geno$APOEb)
sex <- as.factor(geno$sex)
df <- cbind.data.frame(Y, APOE)
```

### Analysis of Variance (ANOVA)

```{r aov}
# Store the ANOVA model summaries in summaries
summaries <- purrr::map(df[, 1:230], ~ summary(aov(.x ~ df$APOE)))
```

#### Correction for Multiple Testing

```{r FDR adjust}
## Extract and adjust p-values for FDR control
# Create an empty list to store the p-values
p_values <- lapply(summaries, function(summary) {
  summary[[1]][["Pr(>F)"]][[1]]
})

# Coerce p_values to dataframe and transpose it
p_values <- t(data.frame(p_values))

# Set row names for p_values
rownames(p_values) <- colnames(Y)

# Calculate the FDR-adjusted p-values
p_values$p_adj <- p.adjust(p_values, method = "fdr")
```


```{r}
# Filter out the non-significant (a=0.05) FDR-adjusted p-values
filter(p_values, p_adj < 0.05)
```

## Classification of metabolites on ApoE class

A function was created to streamline the analysis of multiple (tidy tabular standardised) predictors and response. The function *fit* fits a Logistic Regression, a Decision Tree and a (bagging) Random Forest. It takes the arguments:

-   X = data frame of predictors,

-   Y = response vector,

-   max_depth = maximum depth of the Decision Tree,

-   mtry = the number of random predictors to be used per training of the Random Forest and

-   seed

It first binds *X* and *Y* into *df* which is then split into a training and test set (maintaining class balances). Next it fits the 3 models, returns a dataframe with their repeated (100 times) 10-fold Cross-Validation(CV) performance metrics and plots their ROC curves.

```{r fit function def}
## Function fit
fit <- function(title,
                X,
                y,
                split = 0.8,
                model,
                times,
                ctrl = trainControl(
                  method = "cv",
                  number = 5, classProbs = TRUE,
                  summaryFunction = twoClassSummary
                ),
                grid = NULL,
                seed = 123,
                auc = numeric(times),
                metrics = matrix(
                  nrow = 11,
                  ncol = times
                )) {
  ### Training-test split
  # Merge X and y into df
  df <- data.frame(X, y)

  # Set the seed once before the loop starts
  set.seed(seed)

  # Perform 10-fold CV repeated 100 times
  results <- lapply(1:times, function(i) {
    # Create a matrix with the training indexes
    split_index <- createDataPartition(df$y, p = split, list = FALSE)

    # Create the training set
    train <- df[split_index, ]

    # Create the test set
    test <- df[-split_index, ]

    # Store the test y in labels
    labels <- test$y
    labels.n <- as.numeric(labels) - 1

    # Train the model
    mdl <- train(
      y ~ .,
      train,
      method = model,
      tuneGrid = grid,
      metric = "ROC",
      trControl = ctrl
    )

    # Create a confusion matrix and get performance metrics from caret
    cm <- confusionMatrix(labels, predict(mdl, test), positive = positive_class)

    # Store the model performance metrics in the cumulative data frame
    metrics[, i] <- cm$byClass

    # Store predictions in a list
    yhat_list <- lapply(test, function(x) predict(mdl, x, type = "prob"))

    # Extract the second column of each prediction
    yhat <- sapply(yhat_list, function(x) x[, 2])

    # Calculate the AUC using the extracted predictions
    auc[i] <- pROC::auc(labels.n,
      yhat,
      levels = c(0, 1),
      ci = TRUE,
      boot.n = 100,
      ci.alpha = 0.95,
      stratified = T,
      quiet = T
    )

    return(list(cm$byClass, auc[i]))
  })

  metrics <- data.frame(do.call(rbind, lapply(results, `[[`, 1)), mean(sapply(results, `[[`, 2)))
  names(metrics) <- c(names(cm$byClass), "AUC")
  names(metrics) <- title
  return(metrics)
}
```

### Original data

```{r fit_original}
X <- Y
# Invoke the function
## Logistic Regression
metrics.lr <- fit(
  title = "Logistic Regression",
  X = X,
  y = APOE,
  model = "glm",
  times = 100,
  grid = NULL
)

## Decision Tree
metrics.tree <- fit(
  title = "Decision Tree",
  X = X,
  y = APOE,
  model = "rpart2",
  grid = expand.grid(maxdepth = 5),
  times = 100
)


## Ensemble Classifiers
metrics.rf <- fit(
  title = "Random Forest",
  X = X,
  y = APOE,
  model = "rf",
  grid = expand.grid(mtry = 10),
  times = 100
)

gbm <- train(X, APOE,
  trControl = trainControl(
    method = "cv",
    number = 10,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  ),
  method = "gbm",
  metric = "ROC",
  tuneLength = 10
)
h2o.init()
metrics.gbm <- fit(
  title = "Stochastic Gradient Boosting",
  X = X,
  y = APOE,
  model = "gbm",
  grid = expand.grid(gbm$bestTune),
  times = 100
)

metrics <- cbind(metrics.lr, metrics.tree, metrics.rf, metrics.gbm)
metrics
```

### Using 6 ML-estimated factor scores

```{r filterX, include=FALSE}
set.seed(123)
cov <- cov(X)

# Find redundant features
filter <- RF(cov)

# Filter out redundant features
filtered <- subSet(X, filter)

# Regularized correlation matrix estimation
M <- regcor(filtered)
```

```{r optCor, echo=FALSE, include=FALSE}
# Get the regularized correlation matrix of the filtered dataset
R <- M$optCor

# Get the Guttman bounds for R
Guttman.bounds <- dimGB(R)
```

```{r dimVar, eval=FALSE, include=FALSE}
# Assess the proportion of cumulative variances for 6 factor solutions
dimVAR(R, maxdim = 6)
```

```{r mlfa, include=FALSE}
mlfa <- mlFA(R, m = 6)
thomson <- facScore(filtered, mlfa$Loadings, mlfa$Uniqueness)
```

```{r fit_thomson, warning=FALSE}
# Invoke the function
## Logistic Regression
metrics.lr <- fit(
  title = "Logistic Regression",
  X = thomson,
  y = APOE,
  model = "glm",
  times = 100,
  grid = NULL
)

## Decision Tree
metrics.tree <- fit(
  title = "Decision Tree",
  X = thomson,
  y = APOE,
  model = "rpart2",
  grid = expand.grid(maxdepth = 5),
  times = 100
)


## Ensemble Classifiers
metrics.rf <- fit(
  title = "Random Forest",
  X = thomson,
  y = APOE,
  model = "rf",
  grid = expand.grid(mtry = 10),
  times = 100
)
h2o.init()
gbm <- train(thomson, APOE,
  trControl = trainControl(
    method = "cv",
    number = 10,
    classProbs = TRUE,
    summaryFunction = twoClassSummary
  ),
  method = "gbm_h2o",
  metric = "ROC",
  tuneLength = 10
)

metrics.gbm <- fit(
  title = "Stochastic Gradient Boosting",
  X = thomson,
  y = APOE,
  model = "gbm_h2o",
  grid = expand.grid(gbm$bestTune),
  times = 100
)

metrics.fa <- cbind(metrics.lr, metrics.tree, metrics.rf, metrics.gbm)
metrics.fa
```

## Network Analysis

Code from rags2ridges

```{r preparation}
# Store all observations of Class 1 in C1
C1 <- scale(df[, APOE == "APOE4NO"])
# Store all observations of Class 2 in C2
C2 <- scale(df[, APOE == "APOE4YES"])

# Get the covariance matrices of C1 and C2
S1 <- covML(C1)
S2 <- covML(C2)

# Store them in a list
S <- list(S1 = S1, S2 = S2)

# Get the total number of samples
n <- c(nrow(S1), nrow(S2))

# Create a list of fused covariance matrices T
Ts <- default.target.fused(Slist = S, ns = n, type = "DUPV")
```

```{r, eval=FALSE}
# Get the optimal lambdas per class and fused with 10-fold CV
set.seed(8910)
optf <- optPenalty.fused(
  Ylist = Ys,
  Tlist = Ts,
  lambda = as.matrix(cbind(
    c("ridge1", "fusion"),
    c("fusion", "ridge2")
  )),
  cv.method = "kCV",
  k = 10,
  verbose = FALSE
)
```

```{r sparsify}
# Create a list of the two-class data Y
Ys <- list(C1 = C1, C2 = C2)

load("opt.rda")
Ps <- optf$Plist
# Get the sparsified high precision matrices, correcting FDR at .001
P0s <- sparsify.fused(Ps,
  threshold = "localFDR",
  FDRcut = 0.999,
  verbose = FALSE
)
```

```{r GGMs per class and diff, message=FALSE}
# Merge the sparse high precision matrices
TST <- Union(P0s$C1$sparseParCor, P0s$C2$sparseParCor)
PCclass1 <- TST$M1subset
PCclass2 <- TST$M2subset

# Create a color map per metabolite class
Colors <- rownames(PCclass2)
Colors[grep("Amine", rownames(PCclass2))] <- "lightblue"
Colors[grep("Org.Acid", rownames(PCclass2))] <- "orange"
Colors[grep("Lip", rownames(PCclass2))] <- "yellow"
Colors[grep("Ox.Stress", rownames(PCclass2))] <- "purple"


set.seed(111213)
opar <- par(mfrow = c(1, 3))
# Plot the sparsified ridge matrix of AD Class 2
Coords <- Ugraph(PCclass2,
  type = "fancy", lay = "layout_with_fr",
  Vcolor = Colors, prune = FALSE, Vsize = 7, Vcex = 0.3,
  main = "AD Class 2"
)
# Plot he sparsified ridge matrix of AD Class 1
Ugraph(PCclass1,
  type = "fancy", lay = NULL, coords = Coords,
  Vcolor = Colors, prune = FALSE, Vsize = 7, Vcex = 0.3,
  main = "AD Class 1"
)

# Plot the differential network
DiffGraph(PCclass1, PCclass2,
  lay = NULL, coords = Coords,
  Vcolor = Colors, Vsize = 7, Vcex = 0.3,
  main = "Differential Network"
)
par(opar)
```

```{r centrality}
PC0list <- list(PCclass1 = PCclass1, PCclass2 = PCclass2)
# Get the network statistics
NetStats <- GGMnetworkStats.fused(PC0list)

# Get the centrality degree scores for each class
DegreesAD1 <- data.frame(rownames(NetStats), NetStats$PCclass1.degree)
DegreesAD2 <- data.frame(rownames(NetStats), NetStats$PCclass2.degree)

# Order and show the centrality degree scores
DegreesAD1o <- DegreesAD1[order(DegreesAD1[, 2], decreasing = TRUE), ]
DegreesAD2o <- DegreesAD2[order(DegreesAD2[, 2], decreasing = TRUE), ]
head(DegreesAD1o, 7)
head(DegreesAD2o, 7)
```

```{r communities}
# Get the communities per class
set.seed(141516)
opar <- par(mfrow = c(1, 2))
CommC1 <- Communities(PCclass1,
  Vcolor = Colors,
  Vsize = 7, Vcex = 0.3, main = "Modules AD Class 1"
)

CommC2 <- Communities(PCclass2,
  Vcolor = Colors,
  Vsize = 7, Vcex = 0.3, main = "Modules AD Class 2"
)
par(opar)
```

```{r degree density plot }
# Plot the densities of centrality degree scores
plot(density(DegreesAD1[, 2]),
  col = "blue", xlim = c(-1, 8), xlab = "Degree", main = ""
)
lines(density(DegreesAD2[, 2]),
  col = "red"
)
legenda <- c("AD class 1", "AD class 2")
legend(5, 0.5,
  legend = legenda, 
  lwd = rep(1, 2), lty = rep(1, 2), col = c("blue", "red"), cex = 0.7
)
```

```{r wilcoxon rank sum }
# Perform a Wilcoxon signed rank test to test if the centrality degree scores are different between the classes
wilcox.test(DegreesAD1[, 2],
  DegreesAD2[, 2],
  paired = TRUE, alternative = "less"
)
```
