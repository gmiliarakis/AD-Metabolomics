---
title: "Thesis.GM.ADmetabolites.Code"
output: html.notebook
editor.options: 
  chunk.output.type: console
---

## Package installation and loading

```{r}
##Packages (install and invoke)
pkgs <- c('car',
          'caret',
          'leaps',
          'glm2',
          'MASS',
          'ISLR',
          'tree',
          'randomForest',
          'ROCR',
          'caTools',
          'plotmo',
          'FMradio',
          "RCurl",
          "jsonlite",
          'rags2ridges')
for (pkg in pkgs) {
  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}


##Load library
library(car)
library(caret)
library(glm2)
library(dplyr)
library(rags2ridges)
library(tree)
library(randomForest)
library(FMradio)
library(ROCR)
```

## Data loading

```{r}
##Invoke data, get to know objects
data(ADdata)
objects()
```

## Data preparation

```{r}
## Replace values of ApoEClass with 0 and 1 to facilitate machine learning
y <- as.character(sampleInfo$ApoEClass)
y <- replace(y, y == 'Class 1',0)
y <- replace(y, y == 'Class 2',1)
y <- as.factor(y)
```

## Data preparation for modelling

```{r}
X <- as.matrix(scale(t(ADmetabolites)))
```

```{r}
df <- cbind.data.frame(X,y)
```

Projection to Latent Structures

```{r}
seed <- 1
set.seed(seed)
cov <- cor(X)

#Find redundant features
filter <- RF(cov)

#Filter out redundant features
filtered <- subSet(X, filter)

#Regularized correlation matrix estimation
M <- regcor(filtered)
```

```{r}
#Get the regularized correlation matrix of the filtered dataset
R <- M$optCor

#Get the Guttman bounds for R
Guttman.bounds <- dimGB(R)
```

```{r}
#Perform a Maximum Likelihood (ML) Factor Analysis of 84
mlfa <- mlFA(R, m=6)
loadings <-mlfa$Loadings
thomson <- facScore(filtered,loadings,mlfa$Uniqueness)
```

```{r}
X <- as.matrix(thomson)
```

## Data preparation for modelling

```{r}
df <- cbind.data.frame(X,y)
```

## lr Logistic Regression

```{r}

# Use 10-fold CV to assess model performance
nfolds <- 10

# Calculate the number of observations per fold
fold_size <- nrow(df) %/% nfolds

# Shuffle the row indices
shuffled_indices <- sample(nrow(df))

# Store the classification metrics for each fold
yhat.lr <- vector("list", length = nfolds)
labels <- vector("list", length = nfolds)

# The outer for loop is across the folds
for (i in 1:nfolds) {  
  # Calculate the indices for the test set
  start_index <- (i - 1) * fold_size + 1
  end_index <- i * fold_size
  test_indices <- shuffled_indices[start_index:end_index]
  
  # Calculate the indices for the training set (exclude test indices)
  train_indices <- setdiff(shuffled_indices, test_indices)
  
  # Create the test and training sets
  test <- df[test_indices, ]
  labels[[i]] <- as.numeric(test$y) -1
  train <- df[train_indices, ]
  
  # Fit a logistic regression model
  lr <- glm2(y ~ .,
             data = train,
             family = 'binomial'(link = "logit"),
             control= glm.control(maxit= 1000))
  
  # Predict probabilities for the test data
  yhat.lr[[i]] <- predict(lr, newdata = test, type = 'response')
}


lr.pred <- prediction(yhat.lr,labels)
lr.perf <- performance(pred,"tpr", "fpr")
plot(lr.perf,
     colorize=TRUE,
     avg='threshold',
     lwd=2,
     main='ROC curves from 10-fold cross-validation')
lr.auc <- mean(perf@y.values[[1]])

yhat.lr <- do.call(c, yhat.lr)
yhat.lr <- as.factor(ifelse(yhat.lr>=0.5,1,0))
labels <- as.factor(do.call(c, labels))

cm.lr <- confusionMatrix(labels,yhat.lr, positive='1')
metrics.lr <- data.frame(cm.lr$byClass)
colnames(metrics.lr) <- 'Logistic Regression'
```


## Decision Tree

```{r}
#df <- data.frame(df)
#dtree <- tree(y~., data =df)
#tree.cv <-cv.tree(dtree, FUN= prune.misclass, rand =c(40,87))

# Calculate the number of observations per fold
fold_size <- nrow(df) %/% nfolds

# Shuffle the row indices
shuffled_indices <- sample(nrow(df))

# Store the classification metrics for each fold
yhat.tree <- vector("list", length = nfolds)
labels <- vector("list", length = nfolds)

# The outer for loop is across the folds
for (i in 1:nfolds) {  
  # Calculate the indices for the test set
  start_index <- (i - 1) * fold_size + 1
  end_index <- i * fold_size
  test_indices <- shuffled_indices[start_index:end_index]
  
  # Calculate the indices for the training set (exclude test indices)
  train_indices <- setdiff(shuffled_indices, test_indices)
  
  # Create the test and training sets
  test <- df[test_indices, ]
  labels[[i]] <- as.numeric(test$y) -1
  train <- df[train_indices, ]
  
  #Fit a decision tree 
  train$y <- as.factor(train$y)
  tree <- tree(y~., data =train)
  
  #Predict probabilities for the test data using the selected lambda
  test$y <- as.numeric(test$y)-1
  yhat.tree[[i]] <- as.numeric(predict(tree, newdata=test, type='class'))-1
  
}

#Flatten the 10 vectors into one list
yhat.tree <- do.call(c, yhat.tree)
labels <- do.call(c, labels)

#Create a ROC object using the pROC package
tree.roc <- pROC::roc(yhat.tree,labels)

#Plot the ROC obtained from 10-fold CV
plot(tree.roc)
#Calculate the mean 10-fold CV AUC
tree.auc <- mean(tree.roc$auc)

yhat.tree <- as.factor(yhat.tree)
labels <- as.factor(labels)

cm.tree <- confusionMatrix(labels,yhat.tree, positive='1')
metrics.tree <- data.frame(cm.tree$byClass)
colnames(metrics.tree) <- 'Decision Tree'
```

```{r}
dtree <- tree(y~. -y, data =df)
summary(dtree)
plot(dtree)
text(dtree)
```

## Random Forest

```{r}
# Calculate the number of observations per fold
fold_size <- nrow(df) %/% nfolds

# Shuffle the row indices
shuffled_indices <- sample(nrow(df))

# Store the classification metrics for each fold
yhat.rf <- vector("list", length = nfolds)
labels <- vector("list", length = nfolds)

# The outer for loop is across the folds
for (i in 1:nfolds) {  
  # Calculate the indices for the test set
  start_index <- (i - 1) * fold_size + 1
  end_index <- i * fold_size
  test_indices <- shuffled_indices[start_index:end_index]
  
  # Calculate the indices for the training set (exclude test indices)
  train_indices <- setdiff(shuffled_indices, test_indices)
  
  # Create the test and training sets
  test <- df[test_indices, ]
  labels[[i]] <- as.numeric(test$y) -1
  train <- df[train_indices, ]
  
  #Fit a Random Forest
  rf <- randomForest(y~., data= train, test= test, ntree = 2300, mtry=230)
  
  #Predict probabilities for the test data using the selected lambda
  test$y <- as.numeric(test$y)-1
  yhat.rf[[i]] <- as.numeric(predict(rf, newdata=test, type='class'))-1
  
}

#Flatten the 10 vectors into one list
yhat.rf <- do.call(c, yhat.rf)
labels <- do.call(c, labels)

#Create a ROC object using the pROC package
rf.roc <- pROC::roc(yhat.rf,labels)

#Plot the ROC obtained from 10-fold CV
plot(rf.roc)
#Calculate the mean 10-fold CV AUC
rf.auc <- mean(rf.roc$auc)

yhat.rf <- as.factor(yhat.rf)
labels <- as.factor(labels)

cm.rf <- confusionMatrix(labels,yhat.rf, positive='1')
metrics.rf <- data.frame(cm.rf$byClass)
colnames(metrics.rf) <- 'Random Forest'
```

```{r}
#Create a data frame to store the metrics
metrics <- cbind(metrics.lr,metrics.tree,metrics.rf)


#Display the table of metrics
print(metrics)
```
